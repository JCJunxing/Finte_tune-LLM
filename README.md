# Fine-Tuning Pre-trained Language Models

This repository contains Jupyter notebooks used for fine-tuning three different pre-trained large language models (LLMs): LORA, GEMMA, and MISTRAL. Each notebook is tailored to fine-tune a specific LLM checkpoint using diverse datasets and configurations.

## Notebooks Description

1. **FineTune_Mistral.ipynb**
   - **Purpose**: This notebook is dedicated to fine-tuning the Mistral model. It includes steps for data loading, model adjustment, and training parameters optimized for specific tasks.
   - **Model Details**: Mistral is known for its efficiency in handling large-scale datasets and complex language tasks.

2. **finetune-gemma.ipynb**
   - **Purpose**: Focuses on the fine-tuning of the Gemma model. The notebook outlines the procedure for model configuration, training, and evaluation.
   - **Model Details**: Gemma is designed for high performance in generative tasks and maintains robustness in diverse linguistic contexts.

3. **YT_PEFT_Finetune_Bloom7B_tagger.ipynb**
   - **Purpose**: This notebook is intended for fine-tuning the Bloom7B model with a focus on part-of-speech tagging and entity recognition.
   - **Model Details**: Particularly effective in natural language understanding tasks, Bloom7B can be adapted for various tagging and classification applications.

## Usage

To use these notebooks:
- Ensure you have Jupyter Notebook or JupyterLab installed.
- Open each notebook in Jupyter and follow the instructions contained within to set up the environment and dependencies.
- Each notebook includes detailed steps for training and evaluating the models.

## Requirements

- Python 3.x
- Jupyter Notebook/Lab
- Necessary Python libraries as listed in each notebook.

## Contributing

Contributions to this project are welcome. Please open an issue or submit a pull request if you have suggestions for improvements or have identified issues with the current implementations.

